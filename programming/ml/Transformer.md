'Transformers' are a deep-learning architecture. Basically all the hot new models use a transformer architecture; e.g. 'GPT' stands for 'generative pre-trained *transformer*'. Transformers are the latest in a series of refinements to learning mechanisms that has been ongoing since at least the 1990s.

They are notable faster than their forerunners at ingesting large datasets.

Transformers and many/most other architectures use the concept of weighted [[Vectors (embeddings)]] to encode the meaning of the tokens in training texts. As each token is considered in-context by each layer, the signal for important tokens is amplified, and the signal for less-important tokens is diminished.